{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age        job  marital    education  default housing loan    contact  \\\n",
       "0   56  housemaid  married     basic.4y       no      no   no  telephone   \n",
       "1   57   services  married  high.school  unknown      no   no  telephone   \n",
       "2   37   services  married  high.school       no     yes   no  telephone   \n",
       "3   40     admin.  married     basic.6y       no      no   no  telephone   \n",
       "4   56   services  married  high.school       no      no  yes  telephone   \n",
       "\n",
       "  month day_of_week  ...  campaign  pdays  previous     poutcome emp.var.rate  \\\n",
       "0   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "1   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "2   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "3   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "4   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "\n",
       "   cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "0          93.994          -36.4      4.857       5191.0  no  \n",
       "1          93.994          -36.4      4.857       5191.0  no  \n",
       "2          93.994          -36.4      4.857       5191.0  no  \n",
       "3          93.994          -36.4      4.857       5191.0  no  \n",
       "4          93.994          -36.4      4.857       5191.0  no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 41188, Columns: 21\n"
     ]
    }
   ],
   "source": [
    "# Import pandas for data loading and table manipulation.\n",
    "import pandas as pd\n",
    "\n",
    "# Define the remote CSV file URL containing the bank marketing dataset.\n",
    "url = \"https://breathecode.herokuapp.com/asset/internal-link?id=413&path=bank-marketing-campaign-data.csv\"\n",
    "# Read the CSV file using semicolon as delimiter into a DataFrame.\n",
    "df = pd.read_csv(url, sep=\";\")\n",
    "\n",
    "# Display the first 5 rows for a quick data preview.\n",
    "display(df.head())\n",
    "# Print total number of rows and columns to confirm dataset dimensions.\n",
    "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Overview\n",
    "\n",
    "This notebook builds a **Logistic Regression** model to predict whether a client will subscribe to a term deposit (`yes` or `no`).\n",
    "\n",
    "### What happens in simple steps\n",
    "1. Load the dataset.\n",
    "2. Split the data into inputs (`X`) and target (`y`).\n",
    "3. Split into training and testing sets with balanced classes.\n",
    "4. Convert text columns to numbers safely using a preprocessing pipeline.\n",
    "5. Train a fast Logistic Regression model.\n",
    "6. Measure accuracy on unseen test data.\n",
    "7. Try a faster hyperparameter search to improve the model.\n",
    "\n",
    "\n",
    "### Step 1: Load data\n",
    "- `pd.read_csv(..., sep=';')` reads the CSV file.\n",
    "- We use `sep=';'` because this file uses semicolons.\n",
    "\n",
    "### Step 2: Create features and target\n",
    "- `X = df.drop(columns=['y'])` keeps all input columns.\n",
    "- `y = df['y'].map({'no': 0, 'yes': 1})` converts labels to numbers.\n",
    "  - `0` = no subscription\n",
    "  - `1` = subscription\n",
    "\n",
    "### Step 3: Fast and balanced train/test split\n",
    "- `train_test_split(..., test_size=0.2, stratify=y, random_state=42)` does an 80/20 split.\n",
    "- `stratify=y` keeps class proportions similar in train and test.\n",
    "- This is simpler and faster than manual index splitting.\n",
    "\n",
    "### Step 4: Convert text to numeric (without leakage)\n",
    "- The model cannot learn from raw text directly.\n",
    "- `OneHotEncoder` turns categories (job, month, etc.) into numeric columns.\n",
    "- `ColumnTransformer` applies encoding only to categorical columns.\n",
    "- `Pipeline` keeps preprocessing + model together in one clean workflow.\n",
    "- `handle_unknown='ignore'` avoids errors if a new category appears in test data.\n",
    "\n",
    "### Step 5: Train the baseline model\n",
    "- We use `LogisticRegression(solver='liblinear', max_iter=200)` for fast binary classification.\n",
    "- `model.fit(X_train_raw, y_train)` learns from the training set.\n",
    "\n",
    "### Step 6: Evaluate model quality\n",
    "- `model.predict(X_test_raw)` makes predictions on unseen data.\n",
    "- `accuracy_score(y_test, y_pred)` gives the percentage of correct predictions.\n",
    "- Accuracy around **0.91** means about **91%** of predictions are correct.\n",
    "\n",
    "### Step 7: Faster hyperparameter tuning\n",
    "- Instead of trying every combination (Grid Search), we use `RandomizedSearchCV`.\n",
    "- It tests a small random set of good parameter candidates (`n_iter=6`, `cv=3`).\n",
    "- This usually gives strong results in much less time.\n",
    "\n",
    "---\n",
    "\n",
    "### Why this approach is better\n",
    "- Faster training and tuning.\n",
    "- Cleaner code with one pipeline.\n",
    "- Lower risk of preprocessing mistakes.\n",
    "- Good accuracy with less compute time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (41188, 20) y shape: (41188,)\n",
      "X_train: (32950, 20) X_test: (8238, 20)\n",
      "y_train: (32950,) y_test: (8238,)\n",
      "Accuracy: 0.9132\n"
     ]
    }
   ],
   "source": [
    "# Import pandas for tabular data handling.\n",
    "import pandas as pd\n",
    "# Import transformer to apply preprocessing by column type.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# Import logistic regression classifier for binary prediction.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Import metric to compute test-set accuracy.\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Import helper to split data into train and test sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import pipeline to chain preprocessing and model steps.\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Import one-hot encoder to convert categorical text into numeric vectors.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the local CSV file using semicolon separator.\n",
    "df = pd.read_csv(\"data/raw/bank-marketing-campaign-data.csv\", sep=\";\")\n",
    "\n",
    "# Create feature matrix by removing target column.\n",
    "X = df.drop(columns=[\"y\"])\n",
    "# Map target labels from text (no/yes) to numeric (0/1).\n",
    "y = df[\"y\"].map({\"no\": 0, \"yes\": 1})\n",
    "# Print shapes to verify expected dimensions before splitting.\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "\n",
    "# Split features and target into stratified train/test subsets (80/20).\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Print split shapes to confirm train/test partitioning.\n",
    "print(\"X_train:\", X_train_raw.shape, \"X_test:\", X_test_raw.shape)\n",
    "# Print label vector shapes to confirm target split worked correctly.\n",
    "print(\"y_train:\", y_train.shape, \"y_test:\", y_test.shape)\n",
    "\n",
    "# Identify categorical columns that need one-hot encoding.\n",
    "categorical_cols = X.select_dtypes(include=[\"object\", \"string\"]).columns.tolist()\n",
    "# Build preprocessing step: one-hot encode categorical columns and pass through others.\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True),\n",
    "            categorical_cols,\n",
    "        )\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Create end-to-end pipeline with preprocessing followed by logistic regression.\n",
    "model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\n",
    "            \"clf\",\n",
    "            LogisticRegression(\n",
    "                solver=\"liblinear\",\n",
    "                C=1.0,\n",
    "                max_iter=200,\n",
    "                random_state=42,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit the full pipeline on the training data.\n",
    "model.fit(X_train_raw, y_train)\n",
    "# Predict target values for the unseen test set.\n",
    "y_pred = model.predict(X_test_raw)\n",
    "# Compute accuracy by comparing predictions to true labels.\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "# Print rounded accuracy for quick model quality check.\n",
    "print(\"Accuracy:\", round(acc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Overview\n",
    "\n",
    "This section improves the model by trying different settings automatically and choosing the best one.\n",
    "\n",
    "### Key points\n",
    "- We start with the model pipeline built in the previous cell.\n",
    "- We test different values for `C` (regularization strength).\n",
    "- We also test whether balancing class weights helps.\n",
    "- We do this with `RandomizedSearchCV`, which is faster than checking every combination.\n",
    "- Finally, we keep the best model and check its accuracy on test data.\n",
    "\n",
    "### Why this is useful\n",
    "- It usually gives better or equal performance.\n",
    "- It saves time compared with a full grid search.\n",
    "- It helps find strong settings without manual trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'clf__C': 1, 'clf__max_iter': 500, 'clf__penalty': 'l2', 'clf__solver': 'liblinear'}\n",
      "Best CV Accuracy: 0.9097\n",
      "Test Score: 0.9132\n"
     ]
    }
   ],
   "source": [
    "# Import Python warnings module.\n",
    "import warnings\n",
    "# Hide repeated FutureWarning messages so output stays clean and easy to read.\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Import GridSearchCV to test multiple parameter combinations.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to evaluate.\n",
    "param_grid = {\n",
    "    # Test different regularization strengths.\n",
    "    \"clf__C\": [0.1, 1, 10],\n",
    "    # Keep solver fixed to liblinear for binary logistic regression.\n",
    "    \"clf__solver\": [\"liblinear\"],\n",
    "    # Use L2 penalty only.\n",
    "    \"clf__penalty\": [\"l2\"],\n",
    "    # Use a higher max_iter to help convergence.\n",
    "    \"clf__max_iter\": [500],\n",
    "}\n",
    "\n",
    "# Create GridSearchCV with the existing preprocessing + model pipeline.\n",
    "search = GridSearchCV(\n",
    "    # Use the pipeline created in the previous training cell.\n",
    "    estimator=model,\n",
    "    # Provide all parameter combinations to evaluate.\n",
    "    param_grid=param_grid,\n",
    "    # Use 5-fold cross-validation for robust validation.\n",
    "    cv=5,\n",
    "    # Optimize based on classification accuracy.\n",
    "    scoring=\"accuracy\",\n",
    "    # Run folds/combinations in parallel across available CPU cores.\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit grid search on the training split only.\n",
    "search.fit(X_train_raw, y_train)\n",
    "\n",
    "# Print the best hyperparameter combination found.\n",
    "print(\"Best Params:\", search.best_params_)\n",
    "# Print mean CV accuracy for the best combination.\n",
    "print(\"Best CV Accuracy:\", round(search.best_score_, 4))\n",
    "# Print the final test score of the best estimator.\n",
    "print(\"Test Score:\", round(search.best_estimator_.score(X_test_raw, y_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations: Accuracy Score vs GridSearchCV\n",
    "\n",
    "### 1) Baseline model (`accuracy_score`)\n",
    "- **Test Accuracy:** **91.32%**\n",
    "- The baseline model correctly predicts approximately 91 out of 100 cases on the test set.\n",
    "- This approach is computationally efficient because it uses a single fixed configuration.\n",
    "\n",
    "### 2) Tuned model (`GridSearchCV`)\n",
    "- **Best CV Accuracy (5-fold):** **90.97%**\n",
    "- **Test Score:** **91.32%**\n",
    "- Best hyperparameters: `C=1`, `solver='liblinear'`, `penalty='l2'`, `max_iter=500`.\n",
    "\n",
    "### 3) Comparison and conclusion\n",
    "- **Baseline Test Accuracy vs GridSearchCV Test Score:** **91.32% vs 91.32%**\n",
    "- **Baseline Test Accuracy vs GridSearchCV Best CV Accuracy:** **91.32% vs 90.97%**\n",
    "- The two approaches deliver identical test-set performance in this notebook.\n",
    "- GridSearchCV provides stronger methodological confidence by validating performance across multiple folds.\n",
    "- Recommended interpretation: retain the baseline model for speed-critical workflows, and use GridSearchCV when robust hyperparameter validation is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomizedSearchCV: Code and Output Explanation\n",
    "\n",
    "This cell tunes the Logistic Regression model by testing multiple hyperparameter combinations automatically.\n",
    "\n",
    "### What the code does\n",
    "- Imports `RandomizedSearchCV` and `numpy`.\n",
    "- Defines `param_distributions` to explore different values for:\n",
    "  - `clf__C` (regularization strength from 0.01 to 100 using `np.logspace`).\n",
    "  - `clf__solver` (`liblinear`).\n",
    "  - `clf__penalty` (`l2`).\n",
    "  - `clf__class_weight` (`None` or `balanced`).\n",
    "  - `clf__max_iter` (300 or 600).\n",
    "- Builds `RandomizedSearchCV` with:\n",
    "  - `n_iter=8` (tries 8 random combinations).\n",
    "  - `cv=3` (3-fold cross-validation).\n",
    "  - `scoring='accuracy'` (selects best accuracy).\n",
    "  - `n_jobs=-1` (uses all CPU cores).\n",
    "- Fits the search on training data and prints the best settings and scores.\n",
    "\n",
    "### How to read the output\n",
    "- `Fitting 3 folds for each of 8 candidates, totalling 24 fits`\n",
    "  - Confirms total model trainings performed during cross-validation.\n",
    "- `Best Params: {...}`\n",
    "  - Shows the hyperparameter combination with the highest average CV accuracy.\n",
    "- `Best CV Accuracy: ...`\n",
    "  - Mean accuracy across the 3 validation folds for the best combination.\n",
    "- `Test Score: ...`\n",
    "  - Accuracy of the best model on unseen test data.\n",
    "\n",
    "### Interpretation tip\n",
    "If `Test Score` is close to `Best CV Accuracy`, the tuned model generalizes well and is likely not overfitting heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best Params: {'clf__solver': 'liblinear', 'clf__penalty': 'l2', 'clf__max_iter': 300, 'clf__class_weight': None, 'clf__C': np.float64(0.027825594022071243)}\n",
      "Best CV Accuracy: 0.9096\n",
      "Test Score: 0.9132\n"
     ]
    }
   ],
   "source": [
    "# Import RandomizedSearchCV for faster randomized hyperparameter tuning.\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Import NumPy to generate logarithmically spaced C values.\n",
    "import numpy as np\n",
    "\n",
    "# Define parameter distributions to sample during randomized search.\n",
    "param_distributions = {\n",
    "    # `np.logspace(-2, 2, 10)` creates 10 C values from 0.01 to 100, evenly spaced in log scale (each step multiplies by a constant factor).\n",
    "    # Sample C values on a log scale from 10^-2 to 10^2.\n",
    "    \"clf__C\": np.logspace(-2, 2, 10),\n",
    "    # Keep solver fixed to liblinear for this binary classification setup.\n",
    "    \"clf__solver\": [\"liblinear\"],\n",
    "    # Restrict to l2 penalty for compatibility and stable convergence.\n",
    "    \"clf__penalty\": [\"l2\"],\n",
    "    # Test default class weighting versus balanced class weighting.\n",
    "    \"clf__class_weight\": [None, \"balanced\"],\n",
    "    # Try moderate and higher iteration limits to ensure convergence.\n",
    "    \"clf__max_iter\": [300, 600],\n",
    "}\n",
    "\n",
    "# Create randomized search configured for faster execution.\n",
    "random_search = RandomizedSearchCV(\n",
    "    # Use the pipeline trained in previous steps as the estimator.\n",
    "    estimator=model,\n",
    "    # Pass the parameter distributions defined above.\n",
    "    param_distributions=param_distributions,\n",
    "    # Evaluate 8 random combinations from the search space.\n",
    "    n_iter=8,\n",
    "    # Use 3-fold cross-validation for speed.\n",
    "    cv=3,\n",
    "    # Optimize hyperparameters using accuracy.\n",
    "    scoring=\"accuracy\",\n",
    "    # Use all available CPU cores in parallel.\n",
    "    n_jobs=-1,\n",
    "    # Fix randomness so results are reproducible.\n",
    "    random_state=42,\n",
    "    # Show progress while fitting.\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Fit randomized search on the training split.\n",
    "random_search.fit(X_train_raw, y_train)\n",
    "\n",
    "# Print the best hyperparameter combination found.\n",
    "print(\"Best Params:\", random_search.best_params_)\n",
    "# Print the best average cross-validation accuracy.\n",
    "print(\"Best CV Accuracy:\", round(random_search.best_score_, 4))\n",
    "# Print the test accuracy of the best estimator.\n",
    "print(\"Test Score:\", round(random_search.best_estimator_.score(X_test_raw, y_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of This RandomizedSearchCV Strategy\n",
    "\n",
    "### Pros\n",
    "- Faster than full GridSearchCV because it tests only a random subset of combinations (`n_iter=8`).\n",
    "- Good coverage across scales of regularization strength using `np.logspace(-2, 2, 10)` for `C`.\n",
    "- Uses cross-validation (`cv=3`), so parameter choice is more reliable than a single train/validation split.\n",
    "- Reproducible results due to `random_state=42`.\n",
    "- Efficient runtime with parallel processing (`n_jobs=-1`).\n",
    "\n",
    "### Cons\n",
    "- May miss the true best combination because it does not evaluate every possible parameter set.\n",
    "- With only 8 iterations, search depth is limited for larger parameter spaces.\n",
    "- `cv=3` is faster but less stable than higher folds (e.g., 5-fold) for noisy data.\n",
    "- Restricting to one solver (`liblinear`) and one penalty (`l2`) reduces exploration breadth.\n",
    "- Accuracy-only optimization may overlook other useful metrics (precision, recall, F1, ROC-AUC) depending on business goals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
