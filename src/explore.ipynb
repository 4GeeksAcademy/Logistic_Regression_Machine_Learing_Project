{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://breathecode.herokuapp.com/asset/internal-link?id=413&path=bank-marketing-campaign-data.csv\"\n",
    "df = pd.read_csv(url, sep=\";\")\n",
    "\n",
    "display(df.head())\n",
    "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Overview\n",
    "\n",
    "This notebook builds a **Logistic Regression** model to predict whether a client will subscribe to a term deposit (`yes` or `no`).\n",
    "\n",
    "### What happens in simple steps\n",
    "1. Load the dataset.\n",
    "2. Split the data into inputs (`X`) and target (`y`).\n",
    "3. Split into training and testing sets with balanced classes.\n",
    "4. Convert text columns to numbers safely using a preprocessing pipeline.\n",
    "5. Train a fast Logistic Regression model.\n",
    "6. Measure accuracy on unseen test data.\n",
    "7. Try a faster hyperparameter search to improve the model.\n",
    "\n",
    "\n",
    "### Step 1: Load data\n",
    "- `pd.read_csv(..., sep=';')` reads the CSV file.\n",
    "- We use `sep=';'` because this file uses semicolons.\n",
    "\n",
    "### Step 2: Create features and target\n",
    "- `X = df.drop(columns=['y'])` keeps all input columns.\n",
    "- `y = df['y'].map({'no': 0, 'yes': 1})` converts labels to numbers.\n",
    "  - `0` = no subscription\n",
    "  - `1` = subscription\n",
    "\n",
    "### Step 3: Fast and balanced train/test split\n",
    "- `train_test_split(..., test_size=0.2, stratify=y, random_state=42)` does an 80/20 split.\n",
    "- `stratify=y` keeps class proportions similar in train and test.\n",
    "- This is simpler and faster than manual index splitting.\n",
    "\n",
    "### Step 4: Convert text to numeric (without leakage)\n",
    "- The model cannot learn from raw text directly.\n",
    "- `OneHotEncoder` turns categories (job, month, etc.) into numeric columns.\n",
    "- `ColumnTransformer` applies encoding only to categorical columns.\n",
    "- `Pipeline` keeps preprocessing + model together in one clean workflow.\n",
    "- `handle_unknown='ignore'` avoids errors if a new category appears in test data.\n",
    "\n",
    "### Step 5: Train the baseline model\n",
    "- We use `LogisticRegression(solver='liblinear', max_iter=200)` for fast binary classification.\n",
    "- `model.fit(X_train_raw, y_train)` learns from the training set.\n",
    "\n",
    "### Step 6: Evaluate model quality\n",
    "- `model.predict(X_test_raw)` makes predictions on unseen data.\n",
    "- `accuracy_score(y_test, y_pred)` gives the percentage of correct predictions.\n",
    "- Accuracy around **0.91** means about **91%** of predictions are correct.\n",
    "\n",
    "### Step 7: Faster hyperparameter tuning\n",
    "- Instead of trying every combination (Grid Search), we use `RandomizedSearchCV`.\n",
    "- It tests a small random set of good parameter candidates (`n_iter=6`, `cv=3`).\n",
    "- This usually gives strong results in much less time.\n",
    "\n",
    "---\n",
    "\n",
    "### Why this approach is better\n",
    "- Faster training and tuning.\n",
    "- Cleaner code with one pipeline.\n",
    "- Lower risk of preprocessing mistakes.\n",
    "- Good accuracy with less compute time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 1) Load CSV\n",
    "df = pd.read_csv(\"data/raw/bank-marketing-campaign-data.csv\", sep=\";\")\n",
    "\n",
    "# 2) Define features and target\n",
    "X = df.drop(columns=[\"y\"])\n",
    "y = df[\"y\"].map({\"no\": 0, \"yes\": 1})\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "\n",
    "# 3) Fast stratified split (80/20)\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"X_train:\", X_train_raw.shape, \"X_test:\", X_test_raw.shape)\n",
    "print(\"y_train:\", y_train.shape, \"y_test:\", y_test.shape)\n",
    "\n",
    "# 4) Sparse one-hot preprocessing + fast binary solver\n",
    "categorical_cols = X.select_dtypes(include=[\"object\", \"string\"]).columns.tolist()\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True),\n",
    "            categorical_cols,\n",
    "        )\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\n",
    "            \"clf\",\n",
    "            LogisticRegression(\n",
    "                solver=\"liblinear\",\n",
    "                C=1.0,\n",
    "                max_iter=200,\n",
    "                random_state=42,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 5) Train + evaluate\n",
    "model.fit(X_train_raw, y_train)\n",
    "y_pred = model.predict(X_test_raw)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", round(acc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Overview\n",
    "\n",
    "This section improves the model by trying different settings automatically and choosing the best one.\n",
    "\n",
    "### Key points\n",
    "- We start with the model pipeline built in the previous cell.\n",
    "- We test different values for `C` (regularization strength).\n",
    "- We also test whether balancing class weights helps.\n",
    "- We do this with `RandomizedSearchCV`, which is faster than checking every combination.\n",
    "- Finally, we keep the best model and check its accuracy on test data.\n",
    "\n",
    "### Why this is useful\n",
    "- It usually gives better or equal performance.\n",
    "- It saves time compared with a full grid search.\n",
    "- It helps find strong settings without manual trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python warnings module\n",
    "import warnings\n",
    "# Hide repeated FutureWarning messages so output stays clean and easy to read\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Tool to test many settings and pick the best one\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# List the settings we want to test\n",
    "param_grid = {\n",
    "    # C controls how strong regularization is\n",
    "    \"clf__C\": [0.1, 1, 10],\n",
    "    # Use this solver\n",
    "    \"clf__solver\": [\"liblinear\"],\n",
    "    # Use L2 penalty\n",
    "    \"clf__penalty\": [\"l2\"],\n",
    "    # Give the model enough training steps\n",
    "    \"clf__max_iter\": [500],\n",
    "}\n",
    "\n",
    "# Set up Grid Search with the model pipeline\n",
    "search = GridSearchCV(\n",
    "    # Model from previous cell\n",
    "    estimator=model,\n",
    "    # Settings to test\n",
    "    param_grid=param_grid,\n",
    "    # Split training data into 5 parts\n",
    "    cv=5,\n",
    "    # Compare by accuracy\n",
    "    scoring=\"accuracy\",\n",
    "    # Use all CPU cores for speed\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Train all combinations on training data\n",
    "search.fit(X_train_raw, y_train)\n",
    "\n",
    "# Show the best settings\n",
    "print(\"Best Params:\", search.best_params_)\n",
    "# Show best average score from 5-fold CV\n",
    "print(\"Best CV Accuracy:\", round(search.best_score_, 4))\n",
    "# Test the best model on test data\n",
    "print(\"Test Score:\", round(search.best_estimator_.score(X_test_raw, y_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations: Accuracy Score vs GridSearchCV\n",
    "\n",
    "### 1) Baseline model (`accuracy_score`)\n",
    "- **Test Accuracy:** **91.32%**\n",
    "- The baseline model correctly predicts approximately 91 out of 100 cases on the test set.\n",
    "- This approach is computationally efficient because it uses a single fixed configuration.\n",
    "\n",
    "### 2) Tuned model (`GridSearchCV`)\n",
    "- **Best CV Accuracy (5-fold):** **90.97%**\n",
    "- **Test Score:** **91.32%**\n",
    "- Best hyperparameters: `C=1`, `solver='liblinear'`, `penalty='l2'`, `max_iter=500`.\n",
    "\n",
    "### 3) Comparison and conclusion\n",
    "- **Baseline Test Accuracy vs GridSearchCV Test Score:** **91.32% vs 91.32%**\n",
    "- **Baseline Test Accuracy vs GridSearchCV Best CV Accuracy:** **91.32% vs 90.97%**\n",
    "- The two approaches deliver identical test-set performance in this notebook.\n",
    "- GridSearchCV provides stronger methodological confidence by validating performance across multiple folds.\n",
    "- Recommended interpretation: retain the baseline model for speed-critical workflows, and use GridSearchCV when robust hyperparameter validation is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
